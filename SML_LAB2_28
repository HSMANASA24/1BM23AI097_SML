gradient descent
def gradient_descent(X, y, decay_rate=0.01, n_iterations=1000, initial_learning_rate = 0.01):
    m = len(y)
    theta = np.random.randn(2) 
    
    for i in range(n_iterations):
        gradients = (2/m) * X.T.dot(X.dot(theta) - y)
        learning_rate = initial_learning_rate / (1 + decay_rate * i)
        theta -= learning_rate * gradients
        error = np.mean((X.dot(theta)-y) ** 2)
        
        if error > 0.1:
            i += 1
        break
    return gradients
    return theta
theta_gd = gradient_descent(X, y)
print("\ngradient Descent:")
print(f"Intercept: {theta_gd[0]}, Slope: {theta_gd[1]}")   


#stochastic gradient descent
def stochastic_gradient_descent(X,y,learning_rate=0.001,n_iterations=100000):
    m=len(y)
    theta=np.array([3000,-0.1])
    for iteration in range(n_iterations):
        for i in range(m):
            random_index=np.random.randint(m)
            xi=X[random_index:random_index+1]
            yi=y[random_index:random_index+1]
            gradients=(2/m)*xi.T.dot(xi.dot(theta)-yi)
            theta-=learning_rate*gradients
    return gradients
    return theta
theta_sgd=stochastic_gradient_descent(X,Y)
print("\nstochastic gardient descent:")
print(f"Intercept:{theta_sgd[0]},slope:{theta_sgd[1]}")



from sklearn.linear_model import SGDRegressor
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=42)
sgd_reg = SGDRegressor(max_iter = 1000,tol = 1e-3)
sgd_reg.fit(X_train,y_train)
x_values = np.linspace(0,25,100)
y_pred = sgd_reg.predict(X)
print("Predictions:" ,y_pred)

